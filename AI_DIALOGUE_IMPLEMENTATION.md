# AI-Driven Dialogue System - Implementation Guide

## Overview

This document describes the complete implementation of an AI-driven dialogue system for OpenMW using the OpenRouter API. This system replaces or augments the traditional dialogue filtering system with dynamic, AI-generated responses while maintaining game state consistency.

## Architecture

### Components

The AI dialogue system consists of four main components located in `components/aidialogue/`:

1. **OpenRouterClient** - HTTP client for OpenRouter API
2. **ContextBuilder** - Builds AI prompts from game state
3. **DialogueGenerator** - Orchestrates AI calls and response parsing
4. **CommandInterpreter** - Safely executes AI-generated game commands

### Data Flow

```
Player initiates dialogue
        ↓
ContextBuilder gathers:
  - NPC information (name, race, class, faction, disposition)
  - Player state (level, skills, items)
  - Journal entries (quest states, unlocked knowledge)
  - Conversation history
  - World context (location, weather, time)
        ↓
DialogueGenerator creates structured prompt
        ↓
OpenRouterClient sends request to OpenRouter API
        ↓
AI generates response with embedded commands
        ↓
DialogueGenerator parses response
        ↓
CommandInterpreter validates and executes commands
        ↓
Game state updated, dialogue displayed to player
```

## Key Features

### 1. Journal as Additive Prompt System

The journal system naturally maps to an additive context system:
- Each unlocked journal entry becomes part of the AI's knowledge base
- Quest indices gate what information is available
- As quests progress, the AI gains more context about the world
- Example: After completing quest stage 10, AI knows about events from that stage

### 2. Structured Command System

The AI can issue game commands in two formats:

**JSON Format** (preferred):
```json
{
  "dialogue": "I heard you found Fargoth's ring! He was so grateful.",
  "commands": [
    {"type": "ADD_JOURNAL", "quest_id": "MS_FargothRing", "index": 20, "text": "..."},
    {"type": "MODIFY_DISPOSITION", "amount": 5}
  ]
}
```

**Inline Format** (fallback):
```
I heard you found Fargoth's ring!
[COMMAND:ADD_JOURNAL:MS_FargothRing:20:I told Arrille about finding the ring]
[COMMAND:MODIFY_DISPOSITION:5]
```

### 3. Safe Command Execution

The CommandInterpreter provides multi-layer security:

- **Whitelist**: Only enabled command types can execute
- **Validation**: All parameters validated before execution
- **Limits**: Constraints on values (e.g., disposition changes limited to ±20)
- **Dry-run mode**: Test commands without executing
- **Execution history**: Full audit trail of AI actions

### 4. Hybrid Operation

The system supports multiple modes:

- **AI-Only**: All dialogue generated by AI
- **Hybrid**: AI for exploration, traditional for quest-critical dialogue
- **Traditional**: Fallback to original Filter system on errors

## Component Documentation

### OpenRouterClient

**Purpose**: HTTP client for communicating with OpenRouter API

**Key Methods**:
```cpp
// Set API credentials
void setApiKey(const std::string& apiKey);

// Set app attribution
void setAppInfo(const std::string& appName, const std::string& siteUrl);

// Make chat completion request
APIResponse chatCompletion(
    const std::vector<Message>& messages,
    const GenerationConfig& config
);
```

**Configuration**:
```cpp
GenerationConfig config;
config.model = "meta-llama/llama-3.1-8b-instruct:free";  // Free model
config.temperature = 0.7f;  // Creativity vs consistency
config.maxTokens = 500;     // Response length limit
config.jsonMode = true;     // Request structured output
```

**Models** (via OpenRouter):
- `meta-llama/llama-3.1-8b-instruct:free` - Free, good quality
- `anthropic/claude-3.5-sonnet` - High quality, paid
- `openai/gpt-4o` - High quality, paid
- `google/gemini-2.0-flash-exp:free` - Free, fast

### ContextBuilder

**Purpose**: Transform game state into AI prompts

**Key Methods**:
```cpp
// Build complete dialogue context
DialogueContext buildContext(
    const MWWorld::Ptr& actor,
    const ESM::RefId& topic,
    const MWBase::Journal& journal
);

// Format context as system prompt
std::string formatAsSystemPrompt(const DialogueContext& context);

// Manage conversation history
void addExchange(const std::string& playerInput, const std::string& npcResponse);
void clearHistory();
```

**Context Structure**:
```cpp
struct DialogueContext {
    // NPC
    std::string npcName;
    std::string npcRace, npcClass, npcFaction;
    int disposition;

    // Player
    int playerLevel;
    std::map<std::string, int> playerSkills;

    // Journal (KEY FEATURE!)
    std::vector<JournalEntryInfo> relevantJournalEntries;

    // Conversation
    std::vector<std::pair<std::string, std::string>> recentExchanges;

    // World
    std::string location, weather, timeOfDay;
};
```

### DialogueGenerator

**Purpose**: Orchestrate AI calls and parse responses

**Key Methods**:
```cpp
// Initialize with API client
void initialize(std::shared_ptr<OpenRouterClient> client);

// Generate response to player input
AIResponse generateResponse(
    const DialogueContext& context,
    const std::string& playerInput,
    const GenerationConfig& config
);

// Generate greeting (conversation start)
AIResponse generateGreeting(
    const DialogueContext& context,
    const GenerationConfig& config
);
```

**Response Structure**:
```cpp
struct AIResponse {
    bool success;
    std::string dialogueText;
    std::vector<GameCommand> commands;
    std::vector<std::string> newTopics;
    std::string errorMessage;
    int tokensUsed;
};
```

### CommandInterpreter

**Purpose**: Safely execute AI-generated commands

**Supported Commands**:
- `ADD_JOURNAL` - Add journal entry for quest
- `SET_QUEST_INDEX` - Update quest stage
- `ADD_TOPIC` - Make topic known to player
- `MODIFY_DISPOSITION` - Change NPC disposition (±20 limit)
- `GIVE_ITEM` - Give item to player (disabled by default)
- `TAKE_ITEM` - Remove item from player (disabled by default)
- `SET_GLOBAL` - Set global variable (disabled by default)

**Key Methods**:
```cpp
// Execute single command with validation
bool executeCommand(
    const GameCommand& command,
    const MWWorld::Ptr& actor,
    MWBase::Journal& journal,
    MWBase::World& world
);

// Execute multiple commands
int executeCommands(const std::vector<GameCommand>& commands, ...);

// Validate without executing
ValidationResult validateCommand(const GameCommand& command);

// Enable/disable specific commands
void setCommandEnabled(GameCommand::Type type, bool enabled);
```

**Safety Features**:
```cpp
// Only safe commands enabled by default
CommandInterpreter interpreter;
// ADD_JOURNAL, SET_QUEST_INDEX, ADD_TOPIC, MODIFY_DISPOSITION: ✓ enabled
// GIVE_ITEM, TAKE_ITEM, SET_GLOBAL, START_COMBAT: ✗ disabled

// Enable item giving if desired
interpreter.setCommandEnabled(GameCommand::Type::GIVE_ITEM, true);

// Dry-run mode for testing
interpreter.setDryRun(true);
```

## Configuration

### OpenMW Settings

Add to `openmw.cfg`:

```ini
[AI Dialogue]
# Enable AI dialogue system
ai dialogue enabled = true

# OpenRouter API key
ai dialogue api key = sk-or-v1-...

# Model selection
ai dialogue model = meta-llama/llama-3.1-8b-instruct:free

# Generation parameters
ai dialogue temperature = 0.7
ai dialogue max tokens = 500

# Features
ai dialogue json mode = true
ai dialogue enable item commands = false
ai dialogue max journal entries = 20
ai dialogue max history exchanges = 5
```

### Environment Variables (alternative)

```bash
export OPENROUTER_API_KEY="sk-or-v1-..."
export OPENMW_AI_MODEL="meta-llama/llama-3.1-8b-instruct:free"
```

## Usage Examples

### Basic Integration (Pseudocode)

```cpp
// In DialogueManager initialization
#include <components/aidialogue/openrouterclient.hpp>
#include <components/aidialogue/contextbuilder.hpp>
#include <components/aidialogue/dialoguegenerator.hpp>
#include <components/aidialogue/commandinterpreter.hpp>

// Setup
auto client = std::make_shared<AIDialogue::OpenRouterClient>();
client->setApiKey(Settings::Manager::getString("api key", "AI Dialogue"));
client->setAppInfo("OpenMW", "https://openmw.org");

AIDialogue::DialogueGenerator generator;
generator.initialize(client);

AIDialogue::ContextBuilder contextBuilder;
AIDialogue::CommandInterpreter interpreter;

// When player talks to NPC
AIDialogue::DialogueContext context = contextBuilder.buildContext(
    actor, topic, MWBase::Environment::get().getJournal()
);

AIDialogue::GenerationConfig config;
config.model = Settings::Manager::getString("model", "AI Dialogue");
config.temperature = Settings::Manager::getFloat("temperature", "AI Dialogue");

// Generate AI response
AIDialogue::AIResponse response = generator.generateResponse(
    context, playerInput, config
);

if (response.success) {
    // Display dialogue to player
    showDialogue(response.dialogueText);

    // Execute commands
    int executed = interpreter.executeCommands(
        response.commands,
        actor,
        MWBase::Environment::get().getJournal(),
        MWBase::Environment::get().getWorld()
    );

    // Update conversation history
    contextBuilder.addExchange(playerInput, response.dialogueText);
} else {
    // Fallback to traditional dialogue
    useLegacyDialogue(actor, topic);
}
```

### Example Dialogue Flow

**Scenario**: Player talks to guard after starting "Missing Taxpayer" quest

**Context Built**:
```
NPC: Socucius Ergalla, Imperial Guard, Disposition: 55
Player: Level 3, Combat: 35, Magic: 20, Stealth: 28
Journal: "MS_TaxCollector" index 10 - "Socucius asked me to find Fargoth"
Location: Seyda Neen, Census and Excise Office
```

**Prompt to AI**:
```
You are Socucius Ergalla, an Imperial Guard.
You are professional and helpful (disposition 55/100).

SETTING:
- Location: Census and Excise Office, Seyda Neen

QUEST KNOWLEDGE:
- MS_TaxCollector (Stage 10): "I asked the prisoner to help locate Fargoth,
  who hasn't paid his taxes."

PLAYER INFO:
- Name: Prisoner
- Level: 3

Player says: "Have you seen Fargoth?"

Respond as Socucius Ergalla in JSON format.
```

**AI Response**:
```json
{
  "dialogue": "Yes, I see him around town frequently. Check near the lighthouse or the tradehouse. He seems nervous lately.",
  "commands": [
    {"type": "ADD_TOPIC", "topic_id": "lighthouse"},
    {"type": "MODIFY_DISPOSITION", "amount": 2}
  ]
}
```

**Result**:
- Player sees: "Yes, I see him around town frequently..."
- "lighthouse" topic added to known topics
- Socucius disposition increased by 2
- Conversation continues naturally

## Performance Considerations

### API Latency

- Average response time: 1-3 seconds
- Mitigation strategies:
  - Show "thinking" indicator
  - Cache common responses
  - Use faster models for generic dialogue
  - Pre-generate likely responses during idle time

### Token Limits

- Most models: 4000-8000 token context limit
- Journal entries can be large
- Solutions:
  - Filter only relevant journal entries (by topic/faction)
  - Summarize old entries
  - Limit to most recent N entries (configurable)
  - Use conversation history sparingly

### Cost Management

**Free Models** (OpenRouter):
- `meta-llama/llama-3.1-8b-instruct:free`
- `google/gemini-2.0-flash-exp:free`
- Rate limited but suitable for testing

**Paid Models** (estimated costs):
- GPT-4o: ~$0.005 per dialogue exchange
- Claude 3.5: ~$0.003 per dialogue exchange
- Llama 3.1 70B: ~$0.001 per dialogue exchange

**Cost Reduction**:
- Use free models when possible
- Cache responses for generic questions
- Use AI selectively (main quests = traditional, exploration = AI)
- Run local LLM with llama.cpp (future enhancement)

## Testing

### Unit Tests

Test individual components:

```cpp
// Test JSON parsing
TEST(JSONBuilder, BasicObject) {
    AIDialogue::JSONBuilder builder;
    builder.startObject();
    builder.addString("test", "value");
    builder.endObject();
    ASSERT_EQ(builder.build(), "{\"test\":\"value\"}");
}

// Test command validation
TEST(CommandInterpreter, ValidateDisposition) {
    AIDialogue::CommandInterpreter interp;
    AIDialogue::GameCommand cmd;
    cmd.type = AIDialogue::GameCommand::Type::ModifyDisposition;
    cmd.parameters["amount"] = "50";  // Too large

    auto result = interp.validateCommand(cmd);
    ASSERT_FALSE(result.isValid);
}
```

### Integration Tests

Test with actual dialogue:

1. Start new game
2. Talk to Sellus Gravius
3. Verify greeting is contextually appropriate
4. Ask about "background"
5. Verify response references player's status
6. Check journal updates correctly

### Manual QA Checklist

- [ ] AI responds in character for different NPC personalities
- [ ] Journal entries correctly gate knowledge
- [ ] Quest progression tracked accurately
- [ ] Disposition changes within acceptable range
- [ ] No inappropriate or lore-breaking responses
- [ ] Fallback to traditional dialogue works on errors
- [ ] Performance acceptable (< 3 second response time)
- [ ] Token usage within limits
- [ ] Commands execute safely

## Troubleshooting

### Common Issues

**"OpenRouterClient not configured"**
- Check API key is set in settings
- Verify environment variable OPENROUTER_API_KEY

**"HTTP error: 401"**
- API key invalid or expired
- Get new key from https://openrouter.ai/keys

**"HTTP error: 429"**
- Rate limit exceeded
- Wait and retry, or upgrade to paid tier

**"Invalid JSON response"**
- AI didn't return valid JSON
- Enable fallback parsing for inline commands
- Try different model or adjust temperature

**Dialogue doesn't make sense**
- Journal context may be too limited
- Increase max journal entries
- Check conversation history is being maintained

**Commands not executing**
- Check command type is enabled
- Verify validation passes
- Check execution history for errors

### Debug Mode

Enable verbose logging:

```cpp
// In DialogueGenerator
#define AI_DIALOGUE_DEBUG 1

// Logs will show:
// - Full prompts sent to API
// - Raw API responses
// - Parsed commands
// - Validation results
// - Execution outcomes
```

## Future Enhancements

### Phase 2 Features

1. **Local LLM Integration**
   - Integrate llama.cpp for offline play
   - Run quantized models locally
   - No API costs or latency

2. **Response Caching**
   - Cache AI responses for identical contexts
   - Reduce API calls by 50-70%
   - SQLite-based cache with TTL

3. **Voice Synthesis**
   - Integrate TTS for voiced dialogue
   - Per-NPC voice models
   - Lip-sync animation matching

4. **Fine-tuned Models**
   - Train on Morrowind dialogue corpus
   - Better lore consistency
   - Authentic voice matching

5. **Multi-NPC Conversations**
   - NPCs discuss events with each other
   - Dynamic rumors spread through towns
   - Emergent storytelling

6. **Quest Generation**
   - AI creates procedural side quests
   - Based on player actions and world state
   - Infinite replayability

### Phase 3: Advanced AI

1. **Vector Database for Lore**
   - Embed entire ES lore
   - Retrieve relevant lore for context
   - Ensure consistency

2. **Personality Models**
   - Unique AI personality per NPC class
   - Consistent character voice
   - Memory of past interactions

3. **Consequence Modeling**
   - AI tracks player reputation
   - NPCs remember all interactions
   - Dynamic relationship evolution

4. **Sentiment Analysis**
   - Analyze player input tone
   - Adjust NPC reactions accordingly
   - More immersive interactions

## Contributing

When extending the AI dialogue system:

1. Maintain the security model - validate all AI output
2. Add new command types conservatively
3. Write tests for new functionality
4. Document configuration options
5. Consider token/cost implications
6. Preserve fallback to traditional dialogue

## License

Part of OpenMW project - GPL 3.0

## Credits

- **OpenMW Team** - Game engine and dialogue system
- **OpenRouter** - Unified AI API
- **Anthropic, OpenAI, Meta** - LLM providers

## References

- OpenRouter API: https://openrouter.ai/docs
- OpenMW Dialogue System: `/apps/openmw/mwdialogue/`
- ESM Format Docs: `/components/esm3/`
- Original Analysis: `/AI_DIALOGUE_ENGINE_ANALYSIS.md`
