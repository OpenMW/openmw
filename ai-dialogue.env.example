# ============================================================================
# OpenMW AI Dialogue System Configuration
# ============================================================================
# Copy this file to ai-dialogue.env and configure with your values

# ----------------------------------------------------------------------------
# OpenRouter API Configuration
# ----------------------------------------------------------------------------
OPENROUTER_API_KEY="sk-or-v1-YOUR_KEY_HERE"

# Get your API key from: https://openrouter.ai/keys
# Free tier available with rate limits
# Paid tier for production use

# ----------------------------------------------------------------------------
# Model Selection (Cerebras-optimized for maximum speed)
# ----------------------------------------------------------------------------

# PRIMARY DIALOGUE MODEL
# Cerebras Llama 4 Maverick - Blazing fast at 2,500+ tokens/sec
# Perfect for real-time dialogue generation
OPENROUTER_DIALOGUE_MODEL="meta-llama/llama-4-maverick:free"

# ALTERNATIVE: Llama 3.3 70B for higher quality
# OPENROUTER_DIALOGUE_MODEL="meta-llama/llama-3.3-70b-instruct"

# CONTEXT SUMMARIZATION MODEL
# Used when journal entries exceed token limits
# Qwen3 235B A22B Thinking - Excellent reasoning and summarization
OPENROUTER_SUMMARY_MODEL="qwen/qwen3-235b-a22b-thinking-2507"

# FALLBACK MODEL
# Used when primary models are unavailable
OPENROUTER_FALLBACK_MODEL="meta-llama/llama-3.1-8b-instruct:free"

# ----------------------------------------------------------------------------
# Provider Routing (Multi-tier fallback system)
# ----------------------------------------------------------------------------

# PRIMARY PROVIDERS (tried first, in order)
# Cerebras: Fastest inference (2,700+ tokens/s)
# Groq: Fast and reliable
# Together: Good balance of speed/cost
OPENROUTER_DIALOGUE_PROVIDERS="Cerebras,Groq,Together"

# SUMMARY PROVIDERS (for context compression)
OPENROUTER_SUMMARY_PROVIDERS="Cerebras,Groq,Together,Fireworks"

# FALLBACK PROVIDERS (when primaries fail)
# OpenAI and Anthropic for maximum reliability
OPENROUTER_FALLBACK_PROVIDERS="OpenAI,Anthropic"

# ALLOW AUTOMATIC FALLBACKS
# Set to false to only use specified providers
OPENROUTER_ALLOW_FALLBACKS=true

# IGNORE PROVIDERS (never use these)
# Comma-separated list of providers to exclude
# OPENROUTER_IGNORE_PROVIDERS="SomeSlowProvider,ExpensiveProvider"

# ----------------------------------------------------------------------------
# Request Configuration
# ----------------------------------------------------------------------------

# TIMEOUT (milliseconds)
# How long to wait for API response before failing
OPENROUTER_TIMEOUT_MS=30000

# MAX RETRIES
# Number of retry attempts on failure
OPENROUTER_MAX_RETRIES=3

# RETRY DELAY (milliseconds)
# Wait time between retries (exponential backoff applied)
OPENROUTER_RETRY_DELAY_MS=1000

# ----------------------------------------------------------------------------
# Generation Parameters
# ----------------------------------------------------------------------------

# TEMPERATURE (0.0 - 2.0)
# 0.0 = Deterministic, consistent responses
# 0.7 = Balanced creativity and consistency (recommended)
# 1.5+ = Very creative, less predictable
OPENROUTER_DIALOGUE_TEMPERATURE=0.7
OPENROUTER_SUMMARY_TEMPERATURE=0.1

# MAX TOKENS
# Maximum response length
# Dialogue: 300-500 tokens = 2-4 sentences
# Summary: 2048 tokens for comprehensive summaries
OPENROUTER_DIALOGUE_MAX_TOKENS=500
OPENROUTER_SUMMARY_MAX_TOKENS=2048

# TOP_P (0.0 - 1.0)
# Nucleus sampling - alternative to temperature
# 0.9 = Recommended for dialogue
OPENROUTER_TOP_P=0.9

# FREQUENCY PENALTY (-2.0 - 2.0)
# Reduces repetition of tokens
# 0.3 = Slight reduction in repetition
OPENROUTER_FREQUENCY_PENALTY=0.3

# PRESENCE PENALTY (-2.0 - 2.0)
# Encourages talking about new topics
# 0.2 = Slight encouragement to explore new topics
OPENROUTER_PRESENCE_PENALTY=0.2

# ----------------------------------------------------------------------------
# Attribution (Optional but recommended)
# ----------------------------------------------------------------------------

# HTTP_REFERER
# Your application URL - helps with OpenRouter rankings
OPENROUTER_HTTP_REFERER="https://openmw.org"

# X_TITLE
# Your application name - shown in OpenRouter dashboard
OPENROUTER_X_TITLE="OpenMW AI Dialogue"

# ----------------------------------------------------------------------------
# Feature Flags
# ----------------------------------------------------------------------------

# STREAMING ENABLED
# Stream responses token-by-token (future feature)
OPENROUTER_STREAMING_ENABLED=false

# JSON MODE
# Force structured JSON output
# Recommended: true for reliable command parsing
OPENROUTER_JSON_MODE=true

# ZERO DATA RETENTION (ZDR)
# Prevent OpenRouter from storing your data
# Set to true for privacy-sensitive applications
OPENROUTER_ZDR=false

# ----------------------------------------------------------------------------
# Context Management (Token Limits)
# ----------------------------------------------------------------------------

# CONTEXT SOFT LIMIT (tokens)
# When to start filtering journal entries
# Llama models have 32K-128K context, but we limit for speed
CONTEXT_SOFT_LIMIT=3000

# CONTEXT TRIGGER LIMIT (tokens)
# When to trigger summarization
CONTEXT_TRIGGER_LIMIT=5000

# CONTEXT HARD LIMIT (tokens)
# Maximum context size before emergency truncation
CONTEXT_HARD_LIMIT=8000

# ----------------------------------------------------------------------------
# Journal Entry Filtering
# ----------------------------------------------------------------------------

# MAX JOURNAL ENTRIES
# Maximum number of journal entries to include in context
# More entries = better context but slower responses
MAX_JOURNAL_ENTRIES=20

# JOURNAL RELEVANCE THRESHOLD
# Only include entries relevant to current topic/NPC
# true = More focused context, faster
# false = All entries included, slower but more comprehensive
JOURNAL_RELEVANCE_FILTERING=true

# ----------------------------------------------------------------------------
# Conversation History
# ----------------------------------------------------------------------------

# MAX CONVERSATION HISTORY
# Number of recent exchanges to remember
# More history = better coherence but uses more tokens
MAX_CONVERSATION_HISTORY=5

# CLEAR HISTORY ON TOPIC CHANGE
# Reset conversation when player changes topic
CLEAR_HISTORY_ON_TOPIC_CHANGE=false

# ----------------------------------------------------------------------------
# AI Dialogue System Features
# ----------------------------------------------------------------------------

# ENABLE AI DIALOGUE
# Master switch for AI dialogue system
# false = Use traditional OpenMW dialogue only
AI_DIALOGUE_ENABLED=true

# HYBRID MODE
# Use AI for exploration, traditional for quest-critical dialogue
# true = Hybrid (recommended for first use)
# false = AI for all dialogue
AI_DIALOGUE_HYBRID_MODE=true

# QUEST CRITICAL TOPICS
# Comma-separated list of topics that always use traditional dialogue
# Ensures important quests don't break due to AI errors
QUEST_CRITICAL_TOPICS="main quest,guild advancement,essential quest"

# ----------------------------------------------------------------------------
# Command Execution Safety
# ----------------------------------------------------------------------------

# ENABLE COMMAND EXECUTION
# Allow AI to modify game state
# false = AI generates dialogue only, no game changes
ENABLE_AI_COMMANDS=true

# ALLOWED COMMANDS
# Comma-separated list of allowed command types
# Available: ADD_JOURNAL,SET_QUEST_INDEX,ADD_TOPIC,MODIFY_DISPOSITION,GIVE_ITEM,TAKE_ITEM,SET_GLOBAL
ALLOWED_COMMANDS="ADD_JOURNAL,SET_QUEST_INDEX,ADD_TOPIC,MODIFY_DISPOSITION"

# DANGEROUS COMMANDS (disabled by default)
# Uncomment to enable item/global manipulation
# WARNING: Can break game balance
# ALLOWED_COMMANDS="ADD_JOURNAL,SET_QUEST_INDEX,ADD_TOPIC,MODIFY_DISPOSITION,GIVE_ITEM,TAKE_ITEM,SET_GLOBAL"

# DRY RUN MODE
# Validate commands but don't execute (for testing)
COMMAND_DRY_RUN=false

# DISPOSITION LIMITS
# Maximum disposition change per exchange
# Prevents AI from making NPCs love/hate you instantly
MIN_DISPOSITION_CHANGE=-10
MAX_DISPOSITION_CHANGE=10

# ITEM LIMITS
# Maximum items AI can give/take per command
MAX_ITEMS_PER_COMMAND=5

# ----------------------------------------------------------------------------
# Performance & Optimization
# ----------------------------------------------------------------------------

# RESPONSE CACHE ENABLED
# Cache AI responses for identical contexts
# Reduces API calls and improves response time
RESPONSE_CACHE_ENABLED=true

# CACHE TTL (seconds)
# How long to keep cached responses
# 3600 = 1 hour
RESPONSE_CACHE_TTL=3600

# CACHE MAX SIZE
# Maximum number of cached responses
RESPONSE_CACHE_MAX_SIZE=1000

# PARALLEL REQUESTS
# Number of concurrent AI requests allowed
# Higher = Faster when multiple NPCs talking
# Lower = Less API load
MAX_PARALLEL_REQUESTS=3

# ----------------------------------------------------------------------------
# Logging & Debugging
# ----------------------------------------------------------------------------

# LOG LEVEL
# ERROR, WARN, INFO, DEBUG, TRACE
LOG_LEVEL=INFO

# LOG AI PROMPTS
# Save full prompts sent to AI (for debugging)
# WARNING: Can create large log files
LOG_AI_PROMPTS=false

# LOG AI RESPONSES
# Save raw AI responses (for debugging)
LOG_AI_RESPONSES=false

# LOG COMMAND EXECUTION
# Record all AI commands executed
# Recommended: true for security auditing
LOG_COMMAND_EXECUTION=true

# LOG FILE PATH
# Where to store AI dialogue logs
LOG_FILE_PATH="logs/ai-dialogue.log"

# ----------------------------------------------------------------------------
# Cost Management
# ----------------------------------------------------------------------------

# DAILY REQUEST LIMIT
# Maximum API requests per day (0 = unlimited)
# Prevents unexpected costs
DAILY_REQUEST_LIMIT=0

# MONTHLY COST LIMIT (USD)
# Maximum spending per month (0 = unlimited)
# Automatically disables AI when limit reached
MONTHLY_COST_LIMIT=0.0

# COST TRACKING ENABLED
# Track API usage and costs
COST_TRACKING_ENABLED=true

# ----------------------------------------------------------------------------
# Example Configurations
# ----------------------------------------------------------------------------

# FOR DEVELOPMENT (Free, Fast):
# OPENROUTER_DIALOGUE_MODEL="meta-llama/llama-4-maverick:free"
# OPENROUTER_DIALOGUE_PROVIDERS="Cerebras"
# OPENROUTER_TIMEOUT_MS=15000
# AI_DIALOGUE_HYBRID_MODE=true
# RESPONSE_CACHE_ENABLED=true

# FOR PRODUCTION (Balanced Quality/Cost):
# OPENROUTER_DIALOGUE_MODEL="meta-llama/llama-3.3-70b-instruct"
# OPENROUTER_DIALOGUE_PROVIDERS="Cerebras,Groq,Together"
# OPENROUTER_FALLBACK_PROVIDERS="OpenAI,Anthropic"
# OPENROUTER_TIMEOUT_MS=30000
# AI_DIALOGUE_HYBRID_MODE=false
# RESPONSE_CACHE_ENABLED=true

# FOR MAXIMUM QUALITY (Higher Cost):
# OPENROUTER_DIALOGUE_MODEL="anthropic/claude-3.5-sonnet"
# OPENROUTER_DIALOGUE_PROVIDERS="Anthropic"
# OPENROUTER_FALLBACK_PROVIDERS="OpenAI"
# OPENROUTER_DIALOGUE_TEMPERATURE=0.8
# AI_DIALOGUE_HYBRID_MODE=false
